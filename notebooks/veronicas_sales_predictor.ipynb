{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the Steps\n",
    "\n",
    "---\n",
    "\n",
    "#### **Initial Model Training**\n",
    "\n",
    "Trained five models using **default hyperparameters**:\n",
    "\n",
    "1. **Linear Regression**\n",
    "2. **Decision Tree Regressor**\n",
    "3. **Random Forest Regressor**\n",
    "4. **Gradient Boosting Regressor**\n",
    "5. **XGBoost Regressor**\n",
    "\n",
    "Measured model performance using:\n",
    "- **Mean Squared Error (MSE)**\n",
    "- **R² Score**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Grid Search Hyperparameter Tuning**\n",
    "\n",
    "- Applied **GridSearchCV** to:\n",
    "  - **Random Forest Regressor**\n",
    "  - **Gradient Boosting Regressor**\n",
    "- Optimized their hyperparameters.\n",
    "- Evaluated the models after tuning to check for performance improvements.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Random Search Hyperparameter Tuning**\n",
    "\n",
    "- Used **RandomizedSearchCV** for:\n",
    "  - **Random Forest Regressor**\n",
    "  - **Gradient Boosting Regressor**\n",
    "- Random Search efficiently sampled hyperparameter combinations compared to Grid Search.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation of Results\n",
    "\n",
    "---\n",
    "\n",
    "#### **Best Performing Models**\n",
    "\n",
    "1. **Linear Regression**  \n",
    "   - **MSE**: 1,120,112.41  \n",
    "   - **R² Score**: 0.502  \n",
    "   - Explains about **50.2%** of the variance in sales, making it the **best-performing model** overall.\n",
    "\n",
    "2. **Gradient Boosting Regressor (Random Search)**  \n",
    "   - **MSE**: 1,172,775.89  \n",
    "   - **R² Score**: 0.479  \n",
    "   - After **Random Search** tuning, this model improved and came close to Linear Regression's performance.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Random Forest Regressor**\n",
    "\n",
    "- **Performance Improved** with both **Grid Search** and **Random Search**:\n",
    "  - **Default R² Score**: 0.367  \n",
    "  - **Random Search R² Score**: 0.435  \n",
    "- Tuning helped reduce MSE and improve R², but it still lags behind Linear Regression and Gradient Boosting.\n",
    "\n",
    "---\n",
    "\n",
    "#### **XGBoost Regressor**\n",
    "\n",
    "- **R² Score**: 0.268  \n",
    "- Underperformed due to potential issues with:\n",
    "  - **Hyperparameters** not being optimal.\n",
    "  - **Feature set** not capturing enough patterns for XGBoost to leverage.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Decision Tree Regressor**\n",
    "\n",
    "- **R² Score**: -0.034  \n",
    "- Poor performance likely due to:\n",
    "  - **Overfitting** the training data.\n",
    "  - Failing to **generalize** to new data.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- **Linear Regression** and **Gradient Boosting** performed the best.\n",
    "- **Random Forest** showed improvement with hyperparameter tuning.\n",
    "- **XGBoost** and **Decision Tree** underperformed, suggesting the need for further tuning or feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Overview\n",
    "\n",
    "### Numerical Features\n",
    "\n",
    "1. **`mean_temp_c`** – Mean temperature (continuous)  \n",
    "2. **`total_rain_mm`** – Total rainfall (continuous)  \n",
    "3. **`total_snow_mm`** – Total snowfall (continuous)  \n",
    "4. **`is_holiday`** – Binary indicator (0 or 1)  \n",
    "5. **`is_holiday_prev_1`** – Binary indicator (0 or 1)  \n",
    "6. **`is_holiday_next_1`** – Binary indicator (0 or 1)  \n",
    "7. **`is_holiday_prev_2`** – Binary indicator (0 or 1)  \n",
    "8. **`is_holiday_next_2`** – Binary indicator (0 or 1)  \n",
    "9. **`month`** – Categorical (but can be treated as numerical or encoded)  \n",
    "\n",
    "### Categorical Features\n",
    "\n",
    "1. **`day_of_week`** – Day of the week (categorical: Monday, Tuesday, etc.)  \n",
    "2. **`season`** – Season (categorical: Winter, Spring, Summer, Fall)  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Preprocessing Steps\n",
    "\n",
    "### Numerical Preprocessing\n",
    "\n",
    "For numerical features, apply **standardization** to scale the data. This ensures all numerical features have a mean of 0 and a standard deviation of 1, which helps models like **Linear Regression** perform better.\n",
    "\n",
    "#### **StandardScaler** will be used for:\n",
    "\n",
    "- `mean_temp_c`  \n",
    "- `total_rain_mm`  \n",
    "- `total_snow_mm`  \n",
    "- `month`  \n",
    "\n",
    "### Categorical Preprocessing\n",
    "\n",
    "For categorical features, apply **one-hot encoding** to convert them into binary columns. This is suitable for models like **Linear Regression** and **tree-based models**.\n",
    "\n",
    "#### **OneHotEncoder** will be used for:\n",
    "\n",
    "- `day_of_week`  \n",
    "- `season`  \n",
    "\n",
    "### Binary Features\n",
    "\n",
    "Binary features (0 or 1) do not need scaling or encoding.\n",
    "\n",
    "#### **Binary features**:\n",
    "\n",
    "- `is_holiday`  \n",
    "- `is_holiday_prev_1`  \n",
    "- `is_holiday_next_1`  \n",
    "- `is_holiday_prev_2`  \n",
    "- `is_holiday_next_2`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config.yaml file\n",
    "with open(\"../config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date        day  gross_sales  returns  discounts_comps  net_sales  \\\n",
      "0  2023-02-01  Wednesday       919.07      0.0           -33.35     885.72   \n",
      "1  2023-02-02   Thursday      1463.52      0.0           -20.61    1442.91   \n",
      "2  2023-02-03     Friday      1051.04      0.0            -9.60    1041.44   \n",
      "3  2023-02-04   Saturday      2243.72      0.0           -12.43    2231.29   \n",
      "4  2023-02-05     Sunday      1405.99      0.0           -25.12    1380.87   \n",
      "\n",
      "   gift_card_sales     tax     tip  refunds_by_amount  ...  total_precip_mm  \\\n",
      "0              0.0   84.44   42.35                0.0  ...              0.0   \n",
      "1              0.0  108.76   72.70                0.0  ...              0.0   \n",
      "2              0.0   93.65   49.94                0.0  ...              0.3   \n",
      "3              0.0  176.67  186.98                0.0  ...              0.0   \n",
      "4              0.0   85.04   77.20                0.0  ...              0.0   \n",
      "\n",
      "   total_snow_mm  holiday_name  is_holiday  is_holiday_prev_1  \\\n",
      "0            0.0   Regular Day           0                  0   \n",
      "1            2.0   Regular Day           0                  0   \n",
      "2            4.0   Regular Day           0                  0   \n",
      "3            2.0   Regular Day           0                  0   \n",
      "4            0.0   Regular Day           0                  0   \n",
      "\n",
      "   is_holiday_next_1  is_holiday_prev_2  is_holiday_next_2  temp_category  \\\n",
      "0                  0                  0                  0           Cold   \n",
      "1                  0                  0                  0           Cold   \n",
      "2                  0                  0                  0           Cold   \n",
      "3                  0                  0                  0           Cold   \n",
      "4                  0                  0                  0           Cold   \n",
      "\n",
      "   season  \n",
      "0  Winter  \n",
      "1  Winter  \n",
      "2  Winter  \n",
      "3  Winter  \n",
      "4  Winter  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get the path to the clean data\n",
    "clean_data_path = config['output_data']['cleaned_merged_climate_sales']\n",
    "\n",
    "# Load the clean data\n",
    "ml_df = pd.read_csv(clean_data_path)\n",
    "\n",
    "# Inspect the data\n",
    "print(ml_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'date' is in datetime format\n",
    "ml_df['date'] = pd.to_datetime(ml_df['date'], errors='coerce')\n",
    "\n",
    "# Feature Engineering: Add day of week and month if not already present\n",
    "ml_df['day_of_week'] = ml_df['date'].dt.day_name()\n",
    "ml_df['month'] = ml_df['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "y = ml_df['net_sales']\n",
    "\n",
    "# Features\n",
    "X = ml_df[['mean_temp_c', 'total_rain_mm', 'total_snow_mm', 'is_holiday', \n",
    "        'is_holiday_prev_1', 'is_holiday_next_1', 'is_holiday_prev_2', \n",
    "        'is_holiday_next_2', 'day_of_week', 'month', 'season']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree Regressor': DecisionTreeRegressor(random_state=0),\n",
    "    'Random Forest Regressor': RandomForestRegressor(n_estimators=100, random_state=0),\n",
    "    'Gradient Boosting Regressor': GradientBoostingRegressor(n_estimators=100, random_state=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Train and evaluate the models\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Store the results\n",
    "    results[model_name] = {'MSE': mse, 'R² Score': r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Linear Regression': {'MSE': 1120112.4094821122,\n",
       "  'R² Score': 0.5019548878268173},\n",
       " 'Decision Tree Regressor': {'MSE': 2324804.485278358,\n",
       "  'R² Score': -0.03369760110640829},\n",
       " 'Random Forest Regressor': {'MSE': 1423006.169818243,\n",
       "  'R² Score': 0.36727665770800844},\n",
       " 'Gradient Boosting Regressor': {'MSE': 1202801.463574221,\n",
       "  'R² Score': 0.46518815006713143}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the results\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'model__max_depth': 10,\n",
       "  'model__min_samples_leaf': 2,\n",
       "  'model__min_samples_split': 5,\n",
       "  'model__n_estimators': 200},\n",
       " np.float64(0.41418605028285765),\n",
       " {'model__learning_rate': 0.1,\n",
       "  'model__max_depth': 3,\n",
       "  'model__min_samples_split': 5,\n",
       "  'model__n_estimators': 100},\n",
       " np.float64(0.45097737141212557))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the parameter grid for Random Forest Regressor\n",
    "rf_param_grid = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [10, 20],\n",
    "    'model__min_samples_split': [2, 5],\n",
    "    'model__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Define the parameter grid for Gradient Boosting Regressor\n",
    "gb_param_grid = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1],\n",
    "    'model__max_depth': [3, 5],\n",
    "    'model__min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Create pipelines for the models\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=0))\n",
    "])\n",
    "\n",
    "gb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', GradientBoostingRegressor(random_state=0))\n",
    "])\n",
    "\n",
    "# Perform GridSearchCV for Random Forest Regressor\n",
    "rf_grid_search = GridSearchCV(rf_pipeline, rf_param_grid, cv=5, scoring='r2', verbose=1, n_jobs=-1)\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Perform GridSearchCV for Gradient Boosting Regressor\n",
    "gb_grid_search = GridSearchCV(gb_pipeline, gb_param_grid, cv=5, scoring='r2', verbose=1, n_jobs=-1)\n",
    "gb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and scores for each model\n",
    "rf_best_params = rf_grid_search.best_params_\n",
    "rf_best_score = rf_grid_search.best_score_\n",
    "\n",
    "gb_best_params = gb_grid_search.best_params_\n",
    "gb_best_score = gb_grid_search.best_score_\n",
    "\n",
    "rf_best_params, rf_best_score, gb_best_params, gb_best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Random Forest': {'MSE': 1314337.1948173903, 'R² Score': 0.41559506877629393},\n",
       " 'Gradient Boosting': {'MSE': 1223375.971764633,\n",
       "  'R² Score': 0.45603993141259513}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Random Forest Regressor with the best parameters\n",
    "best_rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=0))\n",
    "])\n",
    "\n",
    "best_rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Train the Gradient Boosting Regressor with the best parameters\n",
    "best_gb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', GradientBoostingRegressor(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        min_samples_split=5,\n",
    "        random_state=0))\n",
    "])\n",
    "\n",
    "best_gb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate both models on the test set\n",
    "rf_y_pred = best_rf_pipeline.predict(X_test)\n",
    "gb_y_pred = best_gb_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "rf_mse = mean_squared_error(y_test, rf_y_pred)\n",
    "rf_r2 = r2_score(y_test, rf_y_pred)\n",
    "\n",
    "gb_mse = mean_squared_error(y_test, gb_y_pred)\n",
    "gb_r2 = r2_score(y_test, gb_y_pred)\n",
    "\n",
    "# Display the results\n",
    "{'Random Forest': {'MSE': rf_mse, 'R² Score': rf_r2},\n",
    " 'Gradient Boosting': {'MSE': gb_mse, 'R² Score': gb_r2}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'XGBoost': {'MSE': 1646640.7665049487, 'R² Score': 0.2678401039748596}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the target variable\n",
    "y = ml_df['net_sales']\n",
    "\n",
    "# Define the features with updated columns\n",
    "X = ml_df[['mean_temp_c', 'total_rain_mm', 'total_snow_mm', 'is_holiday',\n",
    "        'is_holiday_prev_1', 'is_holiday_next_1', 'is_holiday_prev_2',\n",
    "        'is_holiday_next_2', 'day_of_week', 'month', 'season']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['mean_temp_c', 'total_rain_mm', 'total_snow_mm', 'month']\n",
    "categorical_features = ['day_of_week', 'season']\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "num_transformer = StandardScaler()\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Combine preprocessors in a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, numerical_features),\n",
    "        ('cat', cat_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep binary features as they are\n",
    ")\n",
    "\n",
    "# Apply preprocessing to training and testing data separately\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Initialize the XGBoost model with reduced estimators for quicker execution\n",
    "xgb_model = XGBRegressor(n_estimators=50, random_state=0)\n",
    "\n",
    "# Train the XGBoost model on the preprocessed data\n",
    "xgb_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "xgb_y_pred = xgb_model.predict(X_test_preprocessed)\n",
    "\n",
    "# Calculate performance metrics\n",
    "xgb_mse = mean_squared_error(y_test, xgb_y_pred)\n",
    "xgb_r2 = r2_score(y_test, xgb_y_pred)\n",
    "\n",
    "# Display the results for XGBoost\n",
    "{'XGBoost': {'MSE': xgb_mse, 'R² Score': xgb_r2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'model__n_estimators': 100,\n",
       "  'model__min_samples_split': 10,\n",
       "  'model__min_samples_leaf': 4,\n",
       "  'model__max_depth': 10},\n",
       " np.float64(0.4339342127867581),\n",
       " {'model__n_estimators': 100,\n",
       "  'model__min_samples_split': 5,\n",
       "  'model__max_depth': 3,\n",
       "  'model__learning_rate': 0.1},\n",
       " np.float64(0.45097737141212557))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the parameter grid for Random Forest Regressor\n",
    "rf_param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [10, 20, 30, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Define the parameter grid for Gradient Boosting Regressor\n",
    "gb_param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create pipelines for the models\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=0))\n",
    "])\n",
    "\n",
    "gb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', GradientBoostingRegressor(random_state=0))\n",
    "])\n",
    "\n",
    "# Perform RandomizedSearchCV for Random Forest Regressor\n",
    "rf_random_search = RandomizedSearchCV(rf_pipeline, rf_param_grid, n_iter=20, cv=5, scoring='r2', verbose=1, n_jobs=-1, random_state=0)\n",
    "rf_random_search.fit(X_train, y_train)\n",
    "\n",
    "# Perform RandomizedSearchCV for Gradient Boosting Regressor\n",
    "gb_random_search = RandomizedSearchCV(gb_pipeline, gb_param_grid, n_iter=20, cv=5, scoring='r2', verbose=1, n_jobs=-1, random_state=0)\n",
    "gb_random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and scores for each model\n",
    "rf_best_params = rf_random_search.best_params_\n",
    "rf_best_score = rf_random_search.best_score_\n",
    "\n",
    "gb_best_params = gb_random_search.best_params_\n",
    "gb_best_score = gb_random_search.best_score_\n",
    "\n",
    "rf_best_params, rf_best_score, gb_best_params, gb_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Random Forest': {'MSE': 1270920.2990006572, 'R² Score': 0.4348998926173707},\n",
       " 'Gradient Boosting': {'MSE': 1172775.894501888,\n",
       "  'R² Score': 0.4785386743449651}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrain Random Forest Regressor with the best hyperparameters\n",
    "best_rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=30,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=4,\n",
    "        random_state=0))\n",
    "])\n",
    "\n",
    "best_rf_pipeline.fit(X_train, y_train)\n",
    "rf_y_pred = best_rf_pipeline.predict(X_test)\n",
    "rf_mse = mean_squared_error(y_test, rf_y_pred)\n",
    "rf_r2 = r2_score(y_test, rf_y_pred)\n",
    "\n",
    "# Retrain Gradient Boosting Regressor with the best hyperparameters\n",
    "best_gb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        min_samples_split=10,\n",
    "        random_state=0))\n",
    "])\n",
    "\n",
    "best_gb_pipeline.fit(X_train, y_train)\n",
    "gb_y_pred = best_gb_pipeline.predict(X_test)\n",
    "gb_mse = mean_squared_error(y_test, gb_y_pred)\n",
    "gb_r2 = r2_score(y_test, gb_y_pred)\n",
    "\n",
    "# Display the results\n",
    "{'Random Forest': {'MSE': rf_mse, 'R² Score': rf_r2},\n",
    " 'Gradient Boosting': {'MSE': gb_mse, 'R² Score': gb_r2}}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
